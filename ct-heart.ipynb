{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e3c830eb082f48ac8abe439ffcfc51b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62786905a2e5498ab0dfeb4542bb58c1","IPY_MODEL_16a8094818474a0c8f20b8d8fbe67aa5","IPY_MODEL_ce67ec9d287645d7a08ac336b0d63123"],"layout":"IPY_MODEL_013492b898c8400180b22852b7a2a74d"}},"62786905a2e5498ab0dfeb4542bb58c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f564b8c4c5b4b3d900ed0bd6d8e2868","placeholder":"​","style":"IPY_MODEL_35235f79b988463aaad5cd9ec832f90a","value":"  0%"}},"16a8094818474a0c8f20b8d8fbe67aa5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f894d71bf3b41c882004a6a38ce5f1e","max":3038,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed1dd7c1dfbb4b4a89df3fe760e762b8","value":3}},"ce67ec9d287645d7a08ac336b0d63123":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c70f3c72983d45bd87151321ae09e306","placeholder":"​","style":"IPY_MODEL_1dd5a4fa2aa1434ebf6ac3c33cd91eff","value":" 3/3038 [03:15&lt;54:28:40, 64.62s/it]"}},"013492b898c8400180b22852b7a2a74d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f564b8c4c5b4b3d900ed0bd6d8e2868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35235f79b988463aaad5cd9ec832f90a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f894d71bf3b41c882004a6a38ce5f1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed1dd7c1dfbb4b4a89df3fe760e762b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c70f3c72983d45bd87151321ae09e306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dd5a4fa2aa1434ebf6ac3c33cd91eff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2439741,"sourceType":"datasetVersion","datasetId":1476353},{"sourceId":350420,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":292610,"modelId":313255}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohammedyassinn/ct-heart?scriptVersionId=235323058\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport torch\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\nimport torchvision.transforms as tt\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport glob","metadata":{"id":"x9zHX1Cto8CG","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:52:31.648355Z","iopub.execute_input":"2025-04-21T22:52:31.648635Z","iopub.status.idle":"2025-04-21T22:52:35.276604Z","shell.execute_reply.started":"2025-04-21T22:52:31.648613Z","shell.execute_reply":"2025-04-21T22:52:35.275882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SegmentationDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths, mask_paths, transform=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        mask_path = self.mask_paths[idx]\n\n        # Use cv2.imread() to read the image and mask in color (BGR format)\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        mask = cv2.imread(mask_path, cv2.IMREAD_COLOR)\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)  # Pass numpy arrays to albumentations\n            image = augmented['image']\n            mask = augmented['mask']\n\n        # Process the mask to be 0 or 255, like in the video's augmentation step\n        mask = mask / 255.0\n        mask = (mask > 0.5) * 255\n\n        # Convert mask to grayscale *before* converting to tensor\n        mask = cv2.cvtColor(mask.astype(np.uint8), cv2.COLOR_BGR2GRAY)\n        mask = mask / 255.0\n        mask = mask > .5\n        # Normalize the image to [0, 1] range *before* converting to tensor\n        image = image / 255.0\n\n        # Convert to tensor *after* augmentation and normalization\n        image = torch.from_numpy(image).permute(2, 0, 1).float()  # Convert to tensor and change to CHW\n        mask = torch.from_numpy(mask).unsqueeze(0).float()  # Convert mask to tensor\n\n        return image, mask\n","metadata":{"id":"RKuFqKdXCpMk","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:56:00.600293Z","iopub.execute_input":"2025-04-21T22:56:00.600713Z","iopub.status.idle":"2025-04-21T22:56:00.607769Z","shell.execute_reply.started":"2025-04-21T22:56:00.600654Z","shell.execute_reply":"2025-04-21T22:56:00.606787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef create_dataloaders(train_images, train_masks, val_images, val_masks, train_tfms, val_tfms, batch_size=4):\n\n    train_datasets = [SegmentationDataset(train_images, train_masks, transform=tfm) for tfm in train_tfms]\n    train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n\n    val_dataset = SegmentationDataset(val_images, val_masks, transform=val_tfms)\n\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers = 4,pin_memory = True)\n    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,num_workers = 4,pin_memory = True)\n\n    return train_dataloader, val_dataloader","metadata":{"id":"Eo1p8a9RCsUu","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:56:00.82791Z","iopub.execute_input":"2025-04-21T22:56:00.828186Z","iopub.status.idle":"2025-04-21T22:56:00.833192Z","shell.execute_reply.started":"2025-04-21T22:56:00.828164Z","shell.execute_reply":"2025-04-21T22:56:00.832339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(path, split):\n  images = sorted(glob.glob(path + '/*/image/*.png'))\n  masks = sorted(glob.glob(path + '/*/mask/*.png'))\n  print(len(images))\n  print(len(masks))\n  split_size = int(len(images) * split)\n  train_images, val_images = train_test_split(images, test_size = split_size, random_state = 42)\n  train_masks, val_masks = train_test_split(masks, test_size = split_size, random_state = 42)\n  return (train_images, train_masks), (val_images, val_masks)","metadata":{"id":"FA0mwTfi8AFi","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:56:02.015703Z","iopub.execute_input":"2025-04-21T22:56:02.015995Z","iopub.status.idle":"2025-04-21T22:56:02.021162Z","shell.execute_reply.started":"2025-04-21T22:56:02.015974Z","shell.execute_reply":"2025-04-21T22:56:02.02006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(train_images, train_masks), (val_images, val_masks) = load_data('/kaggle/input/ct-heart-segmentation/data/train', 0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:56:02.250786Z","iopub.execute_input":"2025-04-21T22:56:02.251095Z","iopub.status.idle":"2025-04-21T22:56:02.288405Z","shell.execute_reply.started":"2025-04-21T22:56:02.25107Z","shell.execute_reply":"2025-04-21T22:56:02.28758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_masks[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:56:02.653579Z","iopub.execute_input":"2025-04-21T22:56:02.653932Z","iopub.status.idle":"2025-04-21T22:56:02.659087Z","shell.execute_reply.started":"2025-04-21T22:56:02.653906Z","shell.execute_reply":"2025-04-21T22:56:02.658425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\n\ntrain_tfms = [\n    A.Compose([\n        A.Resize(256, 256),\n        # ToTensorV2() # Convert to tensor after resizing\n    ]),\n\n    A.Compose([\n        A.VerticalFlip(p=1.0),\n        A.Resize(256, 256),\n        # ToTensorV2()\n    ]),\n\n    A.Compose([\n        A.Rotate(limit=45, p=1.0),\n        A.Resize(256, 256),\n        # ToTensorV2()\n    ]),\n\n    A.Compose([\n        A.HorizontalFlip(p=1.0),\n        A.Resize(256, 256),\n        # ToTensorV2()\n    ])\n]\n\nval_tfms = A.Compose([\n    A.Resize(256, 256),\n    # ToTensorV2()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:56:03.982764Z","iopub.execute_input":"2025-04-21T22:56:03.983109Z","iopub.status.idle":"2025-04-21T22:56:03.989797Z","shell.execute_reply.started":"2025-04-21T22:56:03.983081Z","shell.execute_reply":"2025-04-21T22:56:03.98889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader, val_dataloader = create_dataloaders(train_images, train_masks, val_images, val_masks, train_tfms, val_tfms, 32)","metadata":{"id":"zBvn5ORVAb0j","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:56:05.782096Z","iopub.execute_input":"2025-04-21T22:56:05.78241Z","iopub.status.idle":"2025-04-21T22:56:05.78696Z","shell.execute_reply.started":"2025-04-21T22:56:05.782367Z","shell.execute_reply":"2025-04-21T22:56:05.786096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage:\nfor images, masks in train_dataloader:\n    print(\"Image batch shape:\", images[0].cpu().numpy().transpose((1, 2, 0)).shape)\n    print(\"Mask batch shape:\", masks[0].cpu().numpy().transpose((1, 2, 0)).shape)\n    break  # Print only the first batch\n","metadata":{"id":"snhtec1cC02A","outputId":"43b2a5ca-b723-4ce4-9e5a-00df7f8543f6","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:56:06.832371Z","iopub.execute_input":"2025-04-21T22:56:06.832752Z","iopub.status.idle":"2025-04-21T22:56:08.870582Z","shell.execute_reply.started":"2025-04-21T22:56:06.832719Z","shell.execute_reply":"2025-04-21T22:56:08.869496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage:\nfor images, masks in train_dataloader:\n    image_mat = images[-1].cpu().numpy().transpose((1, 2, 0))\n    mask_mat = masks[1].cpu().numpy().transpose((1, 2, 0))\n    plt.figure(figsize=(10, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(image_mat)\n    plt.title(\"Image\")\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(mask_mat,cmap='gray' )\n    plt.title(\"Mask\")\n    plt.axis('off')\n\n    plt.show()\n    break;\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:36.685919Z","iopub.execute_input":"2025-04-21T23:03:36.686248Z","iopub.status.idle":"2025-04-21T23:03:38.759119Z","shell.execute_reply.started":"2025-04-21T23:03:36.686221Z","shell.execute_reply":"2025-04-21T23:03:38.758084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# try:\n#     import torch_xla.core.xla_model as xm\n# except:\n#     pass\n\n# def get_default_device():\n#     try:\n#         # Check if TPU is available\n#         return xm.xla_device()\n#     except:\n#         # Fallback to GPU if TPU isn't available, otherwise CPU\n#         if torch.cuda.is_available():\n#             return torch.device('cuda')\n#         else:\n#             return torch.device('cpu')\n\n# device = get_default_device()\n# print(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:41.305988Z","iopub.execute_input":"2025-04-21T23:03:41.306295Z","iopub.status.idle":"2025-04-21T23:03:41.310209Z","shell.execute_reply.started":"2025-04-21T23:03:41.306271Z","shell.execute_reply":"2025-04-21T23:03:41.309202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class DeviceDataLoader():\n#   def __init__(self, dl, device):\n#     self.dl = dl\n#     self.device = device\n\n#   def __iter__(self):\n#     for b in self.dl:\n#         images, masks = b\n#         yield [images.to(self.device), masks.to(self.device)]\n\n#   def __len__(self):\n#     return len(self.dl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:41.461362Z","iopub.execute_input":"2025-04-21T23:03:41.461739Z","iopub.status.idle":"2025-04-21T23:03:41.465204Z","shell.execute_reply.started":"2025-04-21T23:03:41.461671Z","shell.execute_reply":"2025-04-21T23:03:41.464357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_dataloader = DeviceDataLoader(train_dataloader, device)\n# val_dataloader = DeviceDataLoader(val_dataloader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:41.884669Z","iopub.execute_input":"2025-04-21T23:03:41.88513Z","iopub.status.idle":"2025-04-21T23:03:41.88858Z","shell.execute_reply.started":"2025-04-21T23:03:41.8851Z","shell.execute_reply":"2025-04-21T23:03:41.887679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_loss(preds, targets, smooth = 1e-15):\n    # ensure output are float tensors\n    preds = torch.sigmoid(preds)\n    preds = preds.float().view(-1)\n    targets = targets.float().view(-1)\n\n    # ensure intersection\n    intersection = (preds * targets).sum()  # Corrected line\n    dice = (2 * intersection + smooth)/(preds.sum() + targets.sum() + smooth)\n    dice_loss = 1 - dice\n\n    return dice_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:41.948886Z","iopub.execute_input":"2025-04-21T23:03:41.94918Z","iopub.status.idle":"2025-04-21T23:03:41.953929Z","shell.execute_reply.started":"2025-04-21T23:03:41.949155Z","shell.execute_reply":"2025-04-21T23:03:41.953112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def segmentation_accuracy(output, target):\n  preds = torch.sigmoid(output)\n  preds = (preds > 0.5).float()\n  accuracy = (preds == target).float().mean()\n  return accuracy","metadata":{"id":"fJt0U3sl1jPR","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:42.115571Z","iopub.execute_input":"2025-04-21T23:03:42.115909Z","iopub.status.idle":"2025-04-21T23:03:42.119844Z","shell.execute_reply.started":"2025-04-21T23:03:42.115882Z","shell.execute_reply":"2025-04-21T23:03:42.118981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conv2D(num_channel,number_filter,dropout):\n  layer = nn.Sequential(\n      nn.Conv2d(num_channel, number_filter, kernel_size = 3, stride = 1, padding = 'same'),\n      nn.BatchNorm2d(number_filter),\n      nn.ReLU(True),\n      nn.Dropout(dropout)\n  )\n  return layer","metadata":{"id":"zNHziYq25Tx4","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:42.274788Z","iopub.execute_input":"2025-04-21T23:03:42.275083Z","iopub.status.idle":"2025-04-21T23:03:42.279399Z","shell.execute_reply.started":"2025-04-21T23:03:42.27506Z","shell.execute_reply":"2025-04-21T23:03:42.278673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conv2DTranspose(num_channel, number_filter, dropout):\n  layer = nn.Sequential(\n      nn.ConvTranspose2d(num_channel, number_filter, kernel_size = 2, stride = (2,2), padding = 0),\n      nn.BatchNorm2d(number_filter),\n      nn.ReLU(True),\n      nn.Dropout(dropout)\n  )\n  return layer","metadata":{"id":"9qWntgxq_3uu","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:42.446872Z","iopub.execute_input":"2025-04-21T23:03:42.447164Z","iopub.status.idle":"2025-04-21T23:03:42.45135Z","shell.execute_reply.started":"2025-04-21T23:03:42.447142Z","shell.execute_reply":"2025-04-21T23:03:42.450515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conv2dCreate(input_channel, num_filters, dropout):\n  layers = [\n      conv2D(input_channel, num_filters, dropout),\n      conv2D(num_filters, num_filters, dropout),\n\n  ]\n  max_pooling = nn.MaxPool2d(2)\n  return nn.Sequential(*layers), max_pooling","metadata":{"id":"BmdmZ_7r5nsx","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:43.916025Z","iopub.execute_input":"2025-04-21T23:03:43.916335Z","iopub.status.idle":"2025-04-21T23:03:43.920847Z","shell.execute_reply.started":"2025-04-21T23:03:43.916309Z","shell.execute_reply":"2025-04-21T23:03:43.919851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def createDownsampling(num_layers, initial_filter, out_filter, dropout):\n  layers = nn.ModuleList()\n  maxpool_layers = nn.ModuleList()\n  for i in range(num_layers):\n    layer, max_layer = conv2dCreate(initial_filter, out_filter, dropout)\n    layers.append(layer)\n    maxpool_layers.append(max_layer)\n    initial_filter = out_filter\n    out_filter = out_filter * 2\n\n  return layers, maxpool_layers","metadata":{"id":"Hi8MRmxe6dxS","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:44.091818Z","iopub.execute_input":"2025-04-21T23:03:44.092111Z","iopub.status.idle":"2025-04-21T23:03:44.09664Z","shell.execute_reply.started":"2025-04-21T23:03:44.092089Z","shell.execute_reply":"2025-04-21T23:03:44.095803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bottle_neck(num_channel, number_filter, dropout):\n    layers = [\n        conv2D(num_channel, number_filter, dropout),\n        conv2D(number_filter, number_filter, dropout),\n    ]\n    return nn.Sequential(*layers)\n","metadata":{"id":"MPPAOjed6O_-","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:44.245005Z","iopub.execute_input":"2025-04-21T23:03:44.245369Z","iopub.status.idle":"2025-04-21T23:03:44.24943Z","shell.execute_reply.started":"2025-04-21T23:03:44.245329Z","shell.execute_reply":"2025-04-21T23:03:44.248576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def upSamplingCreate(num_layers, initial_filter, dropout):\n  layers = nn.ModuleList()\n  current_filter = initial_filter\n  for i in range(num_layers):\n    current_filter = current_filter // 2\n    layer_transpose = conv2DTranspose(initial_filter, current_filter, dropout)\n    layers.append(layer_transpose)\n    layer_Conv1 = conv2D(initial_filter, current_filter, dropout)\n    layers.append(layer_Conv1)\n    layer_Conv2 = conv2D(current_filter, current_filter, dropout)\n    layers.append(layer_Conv2)\n    initial_filter = current_filter\n  return layers","metadata":{"id":"5v2qwSPEPQHS","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:44.421816Z","iopub.execute_input":"2025-04-21T23:03:44.422163Z","iopub.status.idle":"2025-04-21T23:03:44.426877Z","shell.execute_reply.started":"2025-04-21T23:03:44.422133Z","shell.execute_reply":"2025-04-21T23:03:44.426017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\nclass UNettrain(nn.Module):\n\n  def training_step(self, batch):\n    images, labels = batch\n    out = self(images)\n    loss = dice_loss(out, labels)\n    accuracy = segmentation_accuracy(out, labels)\n    return {\"training_loss\":loss, \"training_accuracy\" : accuracy}\n\n  def validation_step(self, batch):\n    images, labels = batch\n    out = self(images)\n    loss = dice_loss(out, labels)\n    accuracy = segmentation_accuracy(out, labels)\n    return {'val_loss': loss, 'val_accuracy': accuracy}\n\n  def validation_epoch_end(self, outputs):\n    batch_losses = [x['val_loss'] for x in outputs]\n    epoch_loss = torch.stack(batch_losses).mean()\n    batch_accuracies = [x['val_accuracy'] for x in outputs]\n    epoch_acc = torch.stack(batch_accuracies).mean()\n    return {'val_loss': epoch_loss.item(), 'val_accuracy': epoch_acc.item()}\n\n  def epoch_end(self, epoch, result):\n    return \"Epoch [{}],training_loss: {:.4f}, training_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n        epoch, result['training_loss'], result['training_acc'], result['val_loss'], result['val_accuracy'])\n","metadata":{"id":"nxmcQvR5DtJq","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:44.573809Z","iopub.execute_input":"2025-04-21T23:03:44.574153Z","iopub.status.idle":"2025-04-21T23:03:44.580675Z","shell.execute_reply.started":"2025-04-21T23:03:44.574124Z","shell.execute_reply":"2025-04-21T23:03:44.579802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNet(UNettrain):\n    def __init__(self, in_channels, out_channels, num_layers, out_filter, dropout):\n        super().__init__()\n        # Initialize Downsampling layers\n        self.downsampling_conv_layers, self.downsampling_maxpool_layers = createDownsampling(num_layers, in_channels, out_filter, dropout)\n\n\n        # Initialize BottleNeck layers\n        bottle_neck_in_channels =  (2**(num_layers-1)) * out_filter\n        bottle_neck_out_channels = bottle_neck_in_channels * 2\n        self.bottle_neck = bottle_neck(bottle_neck_in_channels, bottle_neck_out_channels, dropout)\n\n        # Initialize Upasampling Layers\n        self.upsampling_conv_layers = upSamplingCreate(num_layers, bottle_neck_out_channels, dropout)\n\n        # Initialize Final Layer\n        self.final_conv_layer = nn.Conv2d(out_filter, out_channels, kernel_size=1)\n    def forward(self, x):\n        skip_connections = []\n\n        # Downsampling\n        for conv_layer, maxpool_layer in zip(self.downsampling_conv_layers, self.downsampling_maxpool_layers):\n            x = conv_layer(x)\n            skip_connections.append(x)\n            x = maxpool_layer(x)\n\n        # BottleNeck\n        x = self.bottle_neck(x)\n\n        # Upsampling\n        skip_connections = list(reversed(skip_connections))\n        for i in range(0, len(self.upsampling_conv_layers), 3):\n          skip = skip_connections[i//3]\n          x = self.upsampling_conv_layers[i](x)\n          x = torch.cat((x, skip), dim=1)\n          x = self.upsampling_conv_layers[i+1](x)\n          x = self.upsampling_conv_layers[i+2](x)\n\n        # Final Convolution layer\n        x = self.final_conv_layer(x)\n        return x","metadata":{"id":"qy083_vDtBXU","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:46.187517Z","iopub.execute_input":"2025-04-21T23:03:46.187925Z","iopub.status.idle":"2025-04-21T23:03:46.197877Z","shell.execute_reply.started":"2025-04-21T23:03:46.187888Z","shell.execute_reply":"2025-04-21T23:03:46.196593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet(3,1,4,64,.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:48.225251Z","iopub.execute_input":"2025-04-21T23:03:48.225605Z","iopub.status.idle":"2025-04-21T23:03:48.464943Z","shell.execute_reply.started":"2025-04-21T23:03:48.225575Z","shell.execute_reply":"2025-04-21T23:03:48.46398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\nfrom torchinfo import summary\nclass UNetTrain(pl.LightningModule):\n    def __init__(self, in_channels, out_channels, num_layers, out_filter, dropout, learning_rate=1e-3):\n        super().__init__()\n        # Initialize UNet architecture here\n        self.downsampling_conv_layers, self.downsampling_maxpool_layers = createDownsampling(num_layers, in_channels, out_filter, dropout)\n        bottle_neck_in_channels =  (2**(num_layers-1)) * out_filter\n        bottle_neck_out_channels = bottle_neck_in_channels * 2\n        self.bottle_neck = bottle_neck(bottle_neck_in_channels, bottle_neck_out_channels, dropout)\n        self.upsampling_conv_layers = upSamplingCreate(num_layers, bottle_neck_out_channels, dropout)\n        self.final_conv_layer = nn.Conv2d(out_filter, out_channels, kernel_size=1)  # Use torchmetrics Dice\n        self.learning_rate = learning_rate\n        self.save_hyperparameters()\n        self.validation_step_outputs = []\n        self.training_step_outputs = []# Add this line\n\n    def forward(self, x):\n        skip_connections = []\n        # Downsampling\n        for conv_layer, maxpool_layer in zip(self.downsampling_conv_layers, self.downsampling_maxpool_layers):\n            x = conv_layer(x)\n            skip_connections.append(x)\n            x = maxpool_layer(x)\n        # BottleNeck\n        x = self.bottle_neck(x)\n        # Upsampling\n        skip_connections = list(reversed(skip_connections))\n        for i in range(0, len(self.upsampling_conv_layers), 3):\n            skip = skip_connections[i//3]\n            x = self.upsampling_conv_layers[i](x)\n            x = torch.cat((x, skip), dim=1)\n            x = self.upsampling_conv_layers[i+1](x)\n            x = self.upsampling_conv_layers[i+2](x)\n        # Final Convolution layer\n        x = self.final_conv_layer(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        images, labels = batch\n        out = self(images)\n        loss = dice_loss(out, labels)  # target needs to be int.\n        acc = segmentation_accuracy(out, labels) \n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, labels = batch\n        out = self(images)\n        loss = dice_loss(out, labels)  # target needs to be int.\n        acc = segmentation_accuracy(out, labels) \n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n        self.validation_step_outputs.append({'val_loss': loss, 'val_acc': acc}) # Append\n\n        return {'val_loss': loss, 'val_acc': acc}\n\n    def on_validation_epoch_end(self): # Changed to on_validation_epoch_end\n        avg_loss = torch.stack([x['val_loss'] for x in self.validation_step_outputs]).mean()\n        avg_acc = torch.stack([x['val_acc'] for x in self.validation_step_outputs]).mean()\n        combined_metric = avg_loss - avg_acc  # Changed to subtraction\n        self.validation_step_outputs.clear()  # Clear the list\n\n        return {'val_loss': avg_loss, 'val_acc': avg_acc}\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \n            mode='min', \n            factor=0.1, \n            patience=10, \n            min_lr=1e-7, \n            verbose=True\n        )\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss'  # This should be the validation metric you want to monitor\n            }}\n\n    def configure_callbacks(self):\n        \n        early_stop_callback = pl.callbacks.EarlyStopping(\n            monitor='val_loss',  # Monitor the combined metric\n            min_delta=0.00,\n            patience=10,\n            verbose=True,\n            mode='min',  # We want to minimize the combined metric\n        )\n        \n        checkpoint_callback = ModelCheckpoint(\n            monitor='val_loss',  # Monitor validation loss\n            filename='best_model',  # Filename for the saved best model\n            save_top_k=1,  # Save only the best model\n            mode='min',  # Save when validation loss is minimized\n            verbose=True\n        )\n\n        lr_monitor = LearningRateMonitor(logging_interval='epoch')\n\n        return [early_stop_callback, checkpoint_callback, lr_monitor]\n        return [early_stop_callback, lr_monitor]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:07:01.138865Z","iopub.execute_input":"2025-04-21T23:07:01.139248Z","iopub.status.idle":"2025-04-21T23:07:01.152433Z","shell.execute_reply.started":"2025-04-21T23:07:01.139224Z","shell.execute_reply":"2025-04-21T23:07:01.151605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage (assuming you have train_dataloader, val_dataloader):\nmodel = UNetTrain(\n    in_channels=3,  # Example: 1 for grayscale CT scans\n    out_channels=1,  # Example: 1 for binary segmentation\n    num_layers=4,\n    out_filter=64,\n    dropout=0,\n    learning_rate=1e-4\n)\ninput_size = (32,3,256,256)\nsummary(model, input_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:03:51.685513Z","iopub.execute_input":"2025-04-21T23:03:51.686008Z","iopub.status.idle":"2025-04-21T23:03:52.772931Z","shell.execute_reply.started":"2025-04-21T23:03:51.685984Z","shell.execute_reply":"2025-04-21T23:03:52.771804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T20:37:13.600956Z","iopub.execute_input":"2025-04-21T20:37:13.601395Z","iopub.status.idle":"2025-04-21T20:37:13.607202Z","shell.execute_reply.started":"2025-04-21T20:37:13.601363Z","shell.execute_reply":"2025-04-21T20:37:13.606059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = pl.Trainer(\n    devices=1,  # or 2 for multiple T4 GPUs\n    accumulate_grad_batches=4,  # Accumulate gradients for 4 steps to simulate a larger batch size\n    max_epochs=40,\n    precision='16-mixed',\n)  # Adjust max_epochs as needed\n# trainer.fit(model, train_dataloader, val_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:56:30.857277Z","iopub.execute_input":"2025-04-21T22:56:30.857638Z","iopub.status.idle":"2025-04-21T22:56:30.890872Z","shell.execute_reply.started":"2025-04-21T22:56:30.857609Z","shell.execute_reply":"2025-04-21T22:56:30.889791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the path to the best model checkpoint\nbest_model_path = trainer.checkpoint_callback.best_model_path\nprint(f\"Best model path: {best_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T00:00:31.300866Z","iopub.execute_input":"2025-04-21T00:00:31.301233Z","iopub.status.idle":"2025-04-21T00:00:31.306168Z","shell.execute_reply.started":"2025-04-21T00:00:31.301199Z","shell.execute_reply":"2025-04-21T00:00:31.305234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Get the best model checkpoint path\nbest_model_path = trainer.checkpoint_callback.best_model_path\nprint(f\"Best model path: {best_model_path}\")\n\n# Define the destination path in /kaggle/working\ndestination_path = \"/kaggle/working/best_model.ckpt\"\n\n# Copy the checkpoint to the destination\nshutil.copy(best_model_path, destination_path)\nprint(f\"Model saved to {destination_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T00:03:30.144812Z","iopub.execute_input":"2025-04-21T00:03:30.145184Z","iopub.status.idle":"2025-04-21T00:03:30.421672Z","shell.execute_reply.started":"2025-04-21T00:03:30.145152Z","shell.execute_reply":"2025-04-21T00:03:30.420968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom tqdm.auto import tqdm\nimport gc\n\ndef fit(model, train_loader, val_loader, epochs, optimizer, patience=10):\n    history = []\n    best_val_loss = float('inf')  # Initialize best validation loss as infinity\n    epochs_without_improvement = 0  # Counter to track the number of epochs with no improvement\n\n    for epoch in range(epochs):\n        model.train()\n        train_results = []\n        for batch in tqdm(train_loader):\n            batch_results = model.training_step(batch)\n            loss = batch_results['training_loss']\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            train_results.append(batch_results)\n            torch.cuda.empty_cache()  # Clear unused memory\n            gc.collect()  # Garbage collection\n\n        # Validation\n        model.eval()\n        val_results = []\n        with torch.no_grad():\n            for batch in tqdm(val_loader):\n                batch_results = model.validation_step(batch)\n                val_results.append(batch_results)\n        prind('finished epoch')\n        result = model.validation_epoch_end(val_results)\n        print(\"finished result\")\n        val_loss = result['val_loss']  # Assuming 'val_loss' is part of the validation result\n        \n        # Early stopping check\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            epochs_without_improvement = 0  # Reset counter if improvement\n        else:\n            epochs_without_improvement += 1\n\n        # If no improvement in `patience` epochs, stop early\n        if epochs_without_improvement >= patience:\n            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n            break\n\n        # Record results for this epoch\n        result['training_loss'] = torch.stack([x['training_loss'] for x in train_results]).mean().item()\n        result['training_acc'] = torch.stack([x['training_accuracy'] for x in train_results]).mean().item()\n        emodel.epoch_end(epoch, result)\n        history.append(result)\n\n    return history\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-20T19:43:05.657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.to(device) # transfer the model to the device.\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T19:38:26.64481Z","iopub.execute_input":"2025-04-20T19:38:26.645185Z","iopub.status.idle":"2025-04-20T19:38:26.721773Z","shell.execute_reply.started":"2025-04-20T19:38:26.645153Z","shell.execute_reply":"2025-04-20T19:38:26.720177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = fit(model, train_dataloader, val_dataloader, 10, torch.optim.Adam(model.parameters()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T19:38:27.52612Z","iopub.execute_input":"2025-04-20T19:38:27.526443Z","execution_failed":"2025-04-20T19:43:05.657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pydicom as dicom","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:54:31.538828Z","iopub.execute_input":"2025-04-21T22:54:31.539124Z","iopub.status.idle":"2025-04-21T22:54:31.972626Z","shell.execute_reply.started":"2025-04-21T22:54:31.539103Z","shell.execute_reply":"2025-04-21T22:54:31.971712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_x = glob.glob(\"/kaggle/input/ct-heart-segmentation/data/test/*/*/*.dcm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:02:38.947798Z","iopub.execute_input":"2025-04-21T23:02:38.948103Z","iopub.status.idle":"2025-04-21T23:02:38.969872Z","shell.execute_reply.started":"2025-04-21T23:02:38.94808Z","shell.execute_reply":"2025-04-21T23:02:38.969157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_test_images(paths):\n    images = []\n    for path in paths:\n        image = dicom.dcmread(path).pixel_array\n        images.append(image)\n    return images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:02:40.073332Z","iopub.execute_input":"2025-04-21T23:02:40.073656Z","iopub.status.idle":"2025-04-21T23:02:40.077673Z","shell.execute_reply.started":"2025-04-21T23:02:40.073629Z","shell.execute_reply":"2025-04-21T23:02:40.076924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_images = load_test_images(test_x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:02:40.305792Z","iopub.execute_input":"2025-04-21T23:02:40.306085Z","iopub.status.idle":"2025-04-21T23:02:42.151449Z","shell.execute_reply.started":"2025-04-21T23:02:40.306064Z","shell.execute_reply":"2025-04-21T23:02:42.150762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_test_image(image):\n    # resize images\n    image = cv2.resize(image, (256,256)) # now shape (H,W)\n    # expand dimension\n    image = np.expand_dims(image, axis = -1) # now shape (H,W,1)\n\n    # normalize image by dividing by 255 but first divide by max value\n    image = (image/np.max(image)) * 255.0\n    return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:16:08.631064Z","iopub.execute_input":"2025-04-21T23:16:08.63137Z","iopub.status.idle":"2025-04-21T23:16:08.635657Z","shell.execute_reply.started":"2025-04-21T23:16:08.631347Z","shell.execute_reply":"2025-04-21T23:16:08.634726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_test_images(images):\n    \"\"\"\n    this function normalize and resize the test_images \n    \"\"\"\n    augmented_images = []\n\n    # loop through each image\n    for image in images:\n        # convert image\n        image = convert_test_image(image)\n        image = image/255.0\n\n        # Concatenate each image 3 times \n        image = np.concatenate([image,image,image],axis = -1) #now shape (H,W,3)\n        \n        image = torch.from_numpy(image).permute(2, 0, 1).float()\n\n        # append augmented image to augmented images list\n        augmented_images.append(image)\n    return augmented_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:16:14.388227Z","iopub.execute_input":"2025-04-21T23:16:14.388554Z","iopub.status.idle":"2025-04-21T23:16:14.393261Z","shell.execute_reply.started":"2025-04-21T23:16:14.38853Z","shell.execute_reply":"2025-04-21T23:16:14.392141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create augmented test images\naugmented_test = augment_test_images(test_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:16:15.904612Z","iopub.execute_input":"2025-04-21T23:16:15.905011Z","iopub.status.idle":"2025-04-21T23:16:16.98448Z","shell.execute_reply.started":"2025-04-21T23:16:15.904979Z","shell.execute_reply":"2025-04-21T23:16:16.983745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create test dataloader\ntest_dataloader = DataLoader(augmented_test, batch_size = 64, shuffle=False,num_workers = 4,pin_memory = True )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:20:02.049523Z","iopub.execute_input":"2025-04-21T23:20:02.049966Z","iopub.status.idle":"2025-04-21T23:20:02.054316Z","shell.execute_reply.started":"2025-04-21T23:20:02.049931Z","shell.execute_reply":"2025-04-21T23:20:02.05349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model_path = '/kaggle/input/myunetmodel/pytorch/default/1/best_model.ckpt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:20:03.823253Z","iopub.execute_input":"2025-04-21T23:20:03.823581Z","iopub.status.idle":"2025-04-21T23:20:03.827196Z","shell.execute_reply.started":"2025-04-21T23:20:03.823552Z","shell.execute_reply":"2025-04-21T23:20:03.826392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"in_channels = 3  # Example value\nout_channels = 1  # Example value\nnum_layers = 4  # Example value\nout_filter = 64  # Example value\ndropout = 0  # Example value\nlearning_rate = 1e-3  # Example value\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:20:04.874666Z","iopub.execute_input":"2025-04-21T23:20:04.87504Z","iopub.status.idle":"2025-04-21T23:20:04.879491Z","shell.execute_reply.started":"2025-04-21T23:20:04.875009Z","shell.execute_reply":"2025-04-21T23:20:04.878502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNetTrain.load_from_checkpoint(\n    best_model_path,\n    in_channels=in_channels,\n    out_channels=out_channels,\n    num_layers=num_layers,\n    out_filter=out_filter,\n    dropout=dropout,\n    learning_rate=learning_rate\n)\n\n# Set to evaluation mode\nmodel.eval()\nmodel.freeze()\nmodel.to('cuda')\nprint(model.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:20:05.073234Z","iopub.execute_input":"2025-04-21T23:20:05.07359Z","iopub.status.idle":"2025-04-21T23:20:05.684695Z","shell.execute_reply.started":"2025-04-21T23:20:05.073557Z","shell.execute_reply":"2025-04-21T23:20:05.6839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nfrom torchvision.transforms.functional import to_pil_image\n\n# Initialize an empty list to store predictions\npredictions_mask = []\npredictions_concatenated = []\n# Disable gradient computation for inference\ni = 0\nwith torch.no_grad():\n    for images in test_dataloader:\n        # If DataLoader provides only images\n        images = images.to(model.device)  # Move to the same device as the model\n        outputs = model(images)\n\n        preds = torch.sigmoid(outputs)\n        # Apply a threshold to convert outputs to binary masks\n        preds = (outputs > 0.5)\n        preds = preds.cpu().numpy().astype(np.int32)\n        preds = preds * 255\n        \n\n        # Convert predictions to PIL images or save them\n        for pred in preds:\n            image = convert_test_image(test_images[i]).astype(np.int32)\n            predicted_mask = pred.transpose(1,2,0)\n            concatenated_image = np.concatenate([image, predicted_mask], axis = 1)\n\n            predictions_mask.append(predicted_mask)\n            predictions_concatenated.append(concatenated_image)\n            os.makedirs('predicted-images',exist_ok = True)\n            cv2.imwrite('predicted-images/image_test_{}.png'.format(i),concatenated_image)\n            i = i + 1\n        print('Finished Batch')\n            \n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:47:54.335964Z","iopub.execute_input":"2025-04-21T23:47:54.336312Z","iopub.status.idle":"2025-04-21T23:48:04.264952Z","shell.execute_reply.started":"2025-04-21T23:47:54.33628Z","shell.execute_reply":"2025-04-21T23:48:04.263937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Define the source directory\nsource_dir = \"/kaggle/working/predicted-images\"  # Update this with the actual folder name\ndestination_zip = \"/kaggle/working/predicted-images.zip\"\n\n# Create a zip file of the directory\nshutil.make_archive(destination_zip.replace('.zip', ''), 'zip', source_dir)\n\nprint(f\"Directory zipped as: {destination_zip}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T23:52:28.923988Z","iopub.execute_input":"2025-04-21T23:52:28.924348Z","iopub.status.idle":"2025-04-21T23:52:29.763024Z","shell.execute_reply.started":"2025-04-21T23:52:28.924319Z","shell.execute_reply":"2025-04-21T23:52:29.76225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}