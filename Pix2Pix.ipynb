{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 47283,
          "sourceType": "datasetVersion",
          "datasetId": 34683
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Pix2Pix",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/modi2009/ComputerVision/blob/GANS/Pix2Pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "vikramtiwari_pix2pix_dataset_path = kagglehub.dataset_download('vikramtiwari/pix2pix-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ycDrvcLpLZqL"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:30.268089Z",
          "iopub.execute_input": "2025-05-03T07:10:30.269217Z",
          "iopub.status.idle": "2025-05-03T07:10:30.273104Z",
          "shell.execute_reply.started": "2025-05-03T07:10:30.26919Z",
          "shell.execute_reply": "2025-05-03T07:10:30.272435Z"
        },
        "id": "iU_iYwjkLZqS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.utils\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:32.847705Z",
          "iopub.execute_input": "2025-05-03T07:10:32.848006Z",
          "iopub.status.idle": "2025-05-03T07:10:41.86211Z",
          "shell.execute_reply.started": "2025-05-03T07:10:32.847965Z",
          "shell.execute_reply": "2025-05-03T07:10:41.861334Z"
        },
        "id": "S5drX-aVLZqT",
        "outputId": "a5269170-a21f-49b1-ce24-5c7e6289b8d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discriminator**"
      ],
      "metadata": {
        "id": "-oEfWXRiLZqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Convolutional Block function\n",
        "def Conv2D(num_channel, number_filter,kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect', dropout = 0):\n",
        "  layer = nn.Sequential(\n",
        "      nn.Conv2d(num_channel, number_filter, kernel_size = kernel_size, stride = stride, padding = padding, padding_mode = padding_mode, bias = False),\n",
        "      nn.BatchNorm2d(number_filter),\n",
        "      nn.LeakyReLU(0.2),\n",
        "      nn.Dropout(dropout)\n",
        "  )\n",
        "  return layer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:41.86317Z",
          "iopub.execute_input": "2025-05-03T07:10:41.863711Z",
          "iopub.status.idle": "2025-05-03T07:10:41.867993Z",
          "shell.execute_reply.started": "2025-05-03T07:10:41.863691Z",
          "shell.execute_reply": "2025-05-03T07:10:41.867294Z"
        },
        "id": "gaCUeypULZqX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Discriminator Model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels = 3, features = [64,128,256,512]):\n",
        "        super().__init__()\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, features[0], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "        for feature in features[1:]:\n",
        "            layer = Conv2D(in_channels, feature, stride = 1 if feature == features[-1] else 2)\n",
        "            layers.append(layer)\n",
        "            in_channels = feature\n",
        "\n",
        "        layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels, 1, kernel_size = 4, stride = 1, padding = 1 , padding_mode = 'reflect'\n",
        "            )\n",
        "        )\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.concat([x,y], dim = 1)\n",
        "        x = self.initial(x)\n",
        "        x = self.model(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:41.86862Z",
          "iopub.execute_input": "2025-05-03T07:10:41.868848Z",
          "iopub.status.idle": "2025-05-03T07:10:41.897143Z",
          "shell.execute_reply.started": "2025-05-03T07:10:41.868824Z",
          "shell.execute_reply": "2025-05-03T07:10:41.896374Z"
        },
        "id": "WRpefMSxLZqX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def testDiscriminator():\n",
        "    x = torch.randn((1, 3, 256, 256))\n",
        "    y = torch.randn((1, 3, 256, 256))\n",
        "    model = Discriminator()\n",
        "    preds = model(x, y)\n",
        "    return preds.shape\n",
        "testDiscriminator()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:41.898676Z",
          "iopub.execute_input": "2025-05-03T07:10:41.898854Z",
          "iopub.status.idle": "2025-05-03T07:10:42.196823Z",
          "shell.execute_reply.started": "2025-05-03T07:10:41.89884Z",
          "shell.execute_reply": "2025-05-03T07:10:42.196119Z"
        },
        "id": "76cVWdoaLZqY",
        "outputId": "e59eaa15-9308-438d-f8e3-e5ba3281c97a"
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([1, 1, 30, 30])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generator**"
      ],
      "metadata": {
        "id": "U4TAL_4uLZqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Convolutional Transpose Block function\n",
        "def ConvTranspose2D(num_channel, number_filter,kernel_size = 4, stride = 2, padding = 1, dropout = 0):\n",
        "  layer = nn.Sequential(\n",
        "      nn.ConvTranspose2d(num_channel, number_filter, kernel_size = kernel_size, stride = stride, padding = padding, bias = False),\n",
        "      nn.BatchNorm2d(number_filter),\n",
        "      nn.ReLU(True),\n",
        "      nn.Dropout(dropout)\n",
        "  )\n",
        "  return layer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:42.198139Z",
          "iopub.execute_input": "2025-05-03T07:10:42.198344Z",
          "iopub.status.idle": "2025-05-03T07:10:42.20266Z",
          "shell.execute_reply.started": "2025-05-03T07:10:42.198328Z",
          "shell.execute_reply": "2025-05-03T07:10:42.201854Z"
        },
        "id": "TQyED1-aLZqa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Discriminator Model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels = 3, features = [64,128,256,512,512,512]):\n",
        "        super().__init__()\n",
        "\n",
        "        # create encoder\n",
        "\n",
        "        # create initial layer of encoder\n",
        "        self.initial_down = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features[0], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        # create rest of encoder\n",
        "        self.layers_down = nn.ModuleList()\n",
        "        for i in range(len(features)):\n",
        "            if i != len(features) - 1 :\n",
        "                layer = Conv2D(features[i], features[i+1], padding = 1)\n",
        "                self.layers_down.append(layer)\n",
        "            else:\n",
        "                layer = Conv2D(features[-1], features[-1], padding = 1)\n",
        "                self.layers_down.append(layer)\n",
        "\n",
        "\n",
        "        # create bottleneck\n",
        "        self.bottelneck = nn.Sequential(\n",
        "            nn.Conv2d(features[-1], features[-1], 4, 2, 1, padding_mode = 'reflect'),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # create decoder\n",
        "        self.layers_up = nn.ModuleList()\n",
        "        # reverse features [512,512,512,256,128,64]\n",
        "        features_up = [features[-1], features[-1]]\n",
        "        features_up.extend(features[::-1])\n",
        "        for i in range(len(features_up)):\n",
        "            if i != len(features_up) - 1 :\n",
        "                dropout = 0\n",
        "                if i <= 2:\n",
        "                    dropout = 0.5\n",
        "                layer = ConvTranspose2D(features_up[i] * 2 if i != 0 else features_up[i], features_up[i+1], dropout = dropout)\n",
        "                self.layers_up.append(layer)\n",
        "        # create last layer of decoder\n",
        "        self.last_up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features_up[-1], in_channels, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder initial\n",
        "        x = self.initial_down(x)\n",
        "\n",
        "        # encoder rest layers and create copy (skip connections) of them\n",
        "        skip_connections = []\n",
        "        for layer in self.layers_down:\n",
        "            x = layer(x)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        # bottleneck\n",
        "        x = self.bottelneck(x)\n",
        "\n",
        "        # decoder first layer\n",
        "        x = self.layers_up[0](x)\n",
        "\n",
        "        # decoder middle layers\n",
        "        skip_connections = list(reversed(skip_connections))\n",
        "\n",
        "        i = 0\n",
        "        for layer in self.layers_up[1:]:\n",
        "            x = layer(torch.cat((x, skip_connections[i]), dim=1))\n",
        "            i = i + 1\n",
        "\n",
        "        # decoder last layer\n",
        "        x = self.last_up(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:43.561247Z",
          "iopub.execute_input": "2025-05-03T07:10:43.562014Z",
          "iopub.status.idle": "2025-05-03T07:10:43.571213Z",
          "shell.execute_reply.started": "2025-05-03T07:10:43.561988Z",
          "shell.execute_reply": "2025-05-03T07:10:43.570549Z"
        },
        "id": "A9htzrlYLZqb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def testGenerator():\n",
        "    x = torch.randn((1, 3, 256, 256))\n",
        "    model = Generator()\n",
        "    preds = model(x)\n",
        "    return preds.shape\n",
        "testGenerator()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:48.00336Z",
          "iopub.execute_input": "2025-05-03T07:10:48.004091Z",
          "iopub.status.idle": "2025-05-03T07:10:48.790389Z",
          "shell.execute_reply.started": "2025-05-03T07:10:48.004067Z",
          "shell.execute_reply": "2025-05-03T07:10:48.789801Z"
        },
        "id": "YOXVdPH_LZqc",
        "outputId": "995b9cce-3253-47f2-bfa3-df13c4b3df2c"
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([1, 3, 256, 256])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and Augment Dataset**"
      ],
      "metadata": {
        "id": "sjmtQPI2LZqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data augmentations\n",
        "both_transform = A.Compose([\n",
        "    A.Resize(256,256),\n",
        "])\n",
        "\n",
        "input_only_transform = A.Compose([\n",
        "    A.Normalize([.5,.5,.5], [.5,.5,.5], max_pixel_value = 255.0),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "target_only_transform = A.Compose([\n",
        "    A.Normalize([.5,.5,.5], [.5,.5,.5], max_pixel_value = 255.0),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:48.791476Z",
          "iopub.execute_input": "2025-05-03T07:10:48.791728Z",
          "iopub.status.idle": "2025-05-03T07:10:48.800735Z",
          "shell.execute_reply.started": "2025-05-03T07:10:48.791712Z",
          "shell.execute_reply": "2025-05-03T07:10:48.80011Z"
        },
        "id": "DcHo4SYbLZqd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MapDataset(Dataset):\n",
        "    def __init__(self, root_dir, both_transform=None, input_only_transform=None, target_only_transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.list_files = os.listdir(self.root_dir)\n",
        "        self.both_transform = both_transform\n",
        "        self.input_only_transform = input_only_transform\n",
        "        self.target_only_transform = target_only_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.list_files[idx])\n",
        "        image = np.array(Image.open(img_path))\n",
        "        input_image = image[:,:600, :]\n",
        "        target_image = image[:,600:, :]\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.both_transform:\n",
        "            augmented = self.both_transform(image=input_image, mask=target_image)\n",
        "            input_image = augmented[\"image\"]\n",
        "            target_image = augmented[\"mask\"]\n",
        "\n",
        "        if self.input_only_transform:\n",
        "            input_image = self.input_only_transform(image=input_image)[\"image\"]\n",
        "\n",
        "        if self.target_only_transform:\n",
        "            target_image = self.target_only_transform(image=target_image)[\"image\"]\n",
        "\n",
        "        return input_image, target_image\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:10:48.801429Z",
          "iopub.execute_input": "2025-05-03T07:10:48.801649Z",
          "iopub.status.idle": "2025-05-03T07:10:48.81013Z",
          "shell.execute_reply.started": "2025-05-03T07:10:48.801634Z",
          "shell.execute_reply": "2025-05-03T07:10:48.809376Z"
        },
        "id": "a865N6KBLZqd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 4"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:01.088104Z",
          "iopub.execute_input": "2025-05-03T07:11:01.088588Z",
          "iopub.status.idle": "2025-05-03T07:11:01.092323Z",
          "shell.execute_reply.started": "2025-05-03T07:11:01.088558Z",
          "shell.execute_reply": "2025-05-03T07:11:01.091543Z"
        },
        "id": "dT4RKcvnLZqd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and create DataLoader\n",
        "train_dataset = MapDataset(\n",
        "    root_dir='/kaggle/input/pix2pix-dataset/maps/maps/train',\n",
        "    both_transform=both_transform,\n",
        "    input_only_transform=input_only_transform,\n",
        "    target_only_transform=target_only_transform,\n",
        ")\n",
        "\n",
        "val_dataset = MapDataset(\n",
        "    root_dir='/kaggle/input/pix2pix-dataset/maps/maps/val',\n",
        "    both_transform=both_transform,\n",
        "    input_only_transform=input_only_transform,\n",
        "    target_only_transform=target_only_transform,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:01.723797Z",
          "iopub.execute_input": "2025-05-03T07:11:01.724616Z",
          "iopub.status.idle": "2025-05-03T07:11:01.731767Z",
          "shell.execute_reply.started": "2025-05-03T07:11:01.724574Z",
          "shell.execute_reply": "2025-05-03T07:11:01.731071Z"
        },
        "id": "4vvgTPCRLZqe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Loss Functions**"
      ],
      "metadata": {
        "id": "xLDbDDTtLZqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize criterion loss\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "L1_LOSS = nn.L1Loss()\n",
        "# discriminator loss function\n",
        "def discriminator_loss(real_preds, fake_preds):\n",
        "  # initialize targets\n",
        "  target_true = torch.ones_like(real_preds)\n",
        "  target_false = torch.zeros_like(fake_preds)\n",
        "\n",
        "  # compute losses\n",
        "  real_loss = criterion(real_preds, target_true)\n",
        "  fake_loss = criterion(fake_preds, target_false)\n",
        "\n",
        "  return (real_loss + fake_loss)/2\n",
        "\n",
        "def generator_loss(fake_preds, fake_image, target_image, lamda_l1):\n",
        "\n",
        "  # initialize target\n",
        "  target_true = torch.ones_like(fake_preds)\n",
        "\n",
        "  # compute loss\n",
        "  gen_loss = criterion(fake_preds, target_true)\n",
        "  l1_loss = L1_LOSS(fake_image, target_image) * lamda_l1\n",
        "  return gen_loss + l1_loss"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:04.057471Z",
          "iopub.execute_input": "2025-05-03T07:11:04.057751Z",
          "iopub.status.idle": "2025-05-03T07:11:04.062765Z",
          "shell.execute_reply.started": "2025-05-03T07:11:04.057733Z",
          "shell.execute_reply": "2025-05-03T07:11:04.062193Z"
        },
        "id": "bsKt8yLMLZqf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Model**"
      ],
      "metadata": {
        "id": "0bAX97jHLZqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
        "from torchinfo import summary\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:06.226052Z",
          "iopub.execute_input": "2025-05-03T07:11:06.226318Z",
          "iopub.status.idle": "2025-05-03T07:11:10.990372Z",
          "shell.execute_reply.started": "2025-05-03T07:11:06.226298Z",
          "shell.execute_reply": "2025-05-03T07:11:10.989828Z"
        },
        "id": "zWQcOPZoLZqg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Pix2Pix(pl.LightningModule):\n",
        "    def __init__(self, gen, disc, lr_gen, lr_disc, lambda_l1,image_save_dir=\"/kaggle/working/image\"):\n",
        "        super().__init__()\n",
        "        self.disc = disc\n",
        "        self.gen = gen\n",
        "        self.lr_gen = lr_gen\n",
        "        self.lr_disc = lr_disc\n",
        "        self.lambda_l1 = lambda_l1\n",
        "        self.automatic_optimization = False  # Enable manual optimization\n",
        "        self.accumulate_grad_batches = 4  # Manual gradient accumulation\n",
        "        self.accumulate_steps = 0  # Counter for accumulated steps\n",
        "        self.image_save_dir = image_save_dir\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Define optimizers for Generator and Discriminator.\"\"\"\n",
        "        opt_gen = optim.Adam(self.gen.parameters(), lr=self.lr_gen, betas=(0.5, 0.999))\n",
        "        opt_disc = optim.Adam(self.disc.parameters(), lr=self.lr_disc, betas=(0.5, 0.999))\n",
        "        return [opt_gen, opt_disc]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_image, target_image = batch\n",
        "        opt_gen, opt_disc = self.optimizers()\n",
        "\n",
        "        # Train Discriminator\n",
        "        y_fake = self(input_image)\n",
        "        real_preds = self.disc(input_image, target_image)\n",
        "        fake_preds = self.disc(input_image, y_fake.detach())\n",
        "\n",
        "        # Calculate discriminator loss\n",
        "        disc_loss = discriminator_loss(real_preds, fake_preds)\n",
        "        opt_disc.zero_grad()\n",
        "        self.manual_backward(disc_loss)\n",
        "        opt_disc.step()\n",
        "\n",
        "        # Train Generator\n",
        "        self.accumulate_steps += 1\n",
        "        fake_preds = self.disc(input_image, y_fake)\n",
        "        gen_loss = generator_loss(fake_preds, y_fake, target_image, self.lambda_l1)\n",
        "\n",
        "        # Accumulate gradients for the generator\n",
        "        opt_gen.zero_grad()\n",
        "        self.manual_backward(gen_loss)\n",
        "\n",
        "        # Perform the optimization step every `accumulate_grad_batches` steps\n",
        "        if self.accumulate_steps == self.accumulate_grad_batches:\n",
        "            opt_gen.step()\n",
        "            self.accumulate_steps = 0  # Reset counter\n",
        "\n",
        "            # Log generator loss\n",
        "            self.log(\"gen_loss\", gen_loss, prog_bar=True, on_epoch=True)\n",
        "\n",
        "            # Save images after every 10th step (or adjust as needed)\n",
        "            if batch_idx % 10 == 0:  # You can adjust this condition as per your requirement\n",
        "                self.save_generated_images(input_image, y_fake, target_image, batch_idx)\n",
        "\n",
        "        # Log discriminator loss\n",
        "        self.log(\"loss_disc\", disc_loss, prog_bar=True, logger=True)\n",
        "        return gen_loss\n",
        "    def save_generated_images(self, input_image, generated_image, target_image, batch_idx):\n",
        "        \"\"\"Function to save images to the directory.\"\"\"\n",
        "        # Convert the tensor images to a format that can be saved (e.g., to range [0, 1] and [0, 255])\n",
        "        input_image = (input_image + 1) / 2  # Denormalize if required\n",
        "        generated_image = (generated_image + 1) / 2  # Denormalize if required\n",
        "        target_image = (target_image + 1) / 2  # Denormalize if required\n",
        "\n",
        "        # Save images using torchvision's save_image\n",
        "        torchvision.utils.save_image(input_image, os.path.join(self.image_save_dir, f\"input_{batch_idx}.png\"))\n",
        "        torchvision.utils.save_image(generated_image, os.path.join(self.image_save_dir, f\"generated_{batch_idx}.png\"))\n",
        "        torchvision.utils.save_image(target_image, os.path.join(self.image_save_dir, f\"target_{batch_idx}.png\"))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:10.991292Z",
          "iopub.execute_input": "2025-05-03T07:11:10.991732Z",
          "iopub.status.idle": "2025-05-03T07:11:11.001489Z",
          "shell.execute_reply.started": "2025-05-03T07:11:10.991705Z",
          "shell.execute_reply": "2025-05-03T07:11:11.000797Z"
        },
        "id": "4ErzkQsxLZqg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Hyperparameters\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "LEARNING_RATE = 2e-4\n",
        "IMAGE_SIZE = 256\n",
        "CHANNELS_IMAGE = 3\n",
        "L1_LAMDA = 100\n",
        "NUM_EPOCH = 500\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:13.158945Z",
          "iopub.execute_input": "2025-05-03T07:11:13.159215Z",
          "iopub.status.idle": "2025-05-03T07:11:13.184366Z",
          "shell.execute_reply.started": "2025-05-03T07:11:13.159196Z",
          "shell.execute_reply": "2025-05-03T07:11:13.18361Z"
        },
        "id": "FeLmUFMDLZqg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "disc = Discriminator(CHANNELS_IMAGE)\n",
        "gen = Generator(CHANNELS_IMAGE)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:19.413803Z",
          "iopub.execute_input": "2025-05-03T07:11:19.414083Z",
          "iopub.status.idle": "2025-05-03T07:11:19.884331Z",
          "shell.execute_reply.started": "2025-05-03T07:11:19.414065Z",
          "shell.execute_reply": "2025-05-03T07:11:19.883733Z"
        },
        "id": "62CtahZeLZqh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pix2Pix(gen, disc, LEARNING_RATE, LEARNING_RATE, L1_LAMDA)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:19.885328Z",
          "iopub.execute_input": "2025-05-03T07:11:19.885547Z",
          "iopub.status.idle": "2025-05-03T07:11:19.889479Z",
          "shell.execute_reply.started": "2025-05-03T07:11:19.885529Z",
          "shell.execute_reply": "2025-05-03T07:11:19.888777Z"
        },
        "id": "TOuD4PRTLZqh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('image',exist_ok = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:21.273847Z",
          "iopub.execute_input": "2025-05-03T07:11:21.274157Z",
          "iopub.status.idle": "2025-05-03T07:11:21.277758Z",
          "shell.execute_reply.started": "2025-05-03T07:11:21.274138Z",
          "shell.execute_reply": "2025-05-03T07:11:21.27715Z"
        },
        "id": "UJxcgSPhLZqh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(\n",
        "    devices=1,  # or 2 for multiple T4 GPUs # Accumulate gradients for 4 steps to simulate a larger batch size\n",
        "    max_epochs=100,\n",
        "    precision='16-mixed',\n",
        ")  # Adjust max_epochs as needed\n",
        "trainer.fit(model, train_loader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T07:11:22.527421Z",
          "iopub.execute_input": "2025-05-03T07:11:22.528057Z",
          "iopub.status.idle": "2025-05-03T08:00:52.048091Z",
          "shell.execute_reply.started": "2025-05-03T07:11:22.528036Z",
          "shell.execute_reply": "2025-05-03T08:00:52.047449Z"
        },
        "id": "uAiPeYR8LZqh",
        "outputId": "bbd289b9-ccb5-45c6-f63e-6993dfe50347",
        "colab": {
          "referenced_widgets": [
            "24efd430209f437e87d643b89dbc37b6"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-05-03 07:11:24.432817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746256284.626989      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746256284.685031      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24efd430209f437e87d643b89dbc37b6"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "CFjAjKRzLZqi"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}