{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1377922,"sourceType":"datasetVersion","datasetId":803742}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import important libraries\nimport numpy as np\nfrom math import log2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.utils\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport os\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.utils import save_image\nfrom torchvision import transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:17:55.246961Z","iopub.execute_input":"2025-05-05T10:17:55.247207Z","iopub.status.idle":"2025-05-05T10:17:55.251927Z","shell.execute_reply.started":"2025-05-05T10:17:55.247190Z","shell.execute_reply":"2025-05-05T10:17:55.251106Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# **Create Helper Functions and Classes**","metadata":{}},{"cell_type":"code","source":"factors = [1, 1, 1, 1, 1/2, 1/4, 1/18, 1/16, 1/32]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:17:57.731958Z","iopub.execute_input":"2025-05-05T10:17:57.732325Z","iopub.status.idle":"2025-05-05T10:17:57.735964Z","shell.execute_reply.started":"2025-05-05T10:17:57.732303Z","shell.execute_reply":"2025-05-05T10:17:57.735280Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class WSConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, gain = 2):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,)\n        self.scale = (gain / (in_channels * (kernel_size**2))) ** 0.5\n        self.bias = self.conv.bias\n        self.conv.bias = None\n        nn.init.normal_(self.conv.weight)\n        nn.init.zeros_(self.bias)\n    def forward(self, x):\n        x = self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:17:58.147893Z","iopub.execute_input":"2025-05-05T10:17:58.148474Z","iopub.status.idle":"2025-05-05T10:17:58.153638Z","shell.execute_reply.started":"2025-05-05T10:17:58.148448Z","shell.execute_reply":"2025-05-05T10:17:58.152838Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class WSConvTranspose2d(nn.Module):\n    def __init__(self, z_dim, in_channels, kernel_size, stride, padding, gain = 2):\n        super().__init__()\n        self.convTranspose = nn.ConvTranspose2d(z_dim, in_channels, kernel_size, stride, padding,)\n        self.scale = (gain / (z_dim * kernel_size**2)) ** 0.5\n        self.bias = self.convTranspose.bias\n        self.convTranspose.bias = None\n        nn.init.normal_(self.convTranspose.weight)\n        nn.init.zeros_(self.bias)\n    def forward(self, x):\n        x = self.convTranspose(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:17:58.379945Z","iopub.execute_input":"2025-05-05T10:17:58.380210Z","iopub.status.idle":"2025-05-05T10:17:58.385283Z","shell.execute_reply.started":"2025-05-05T10:17:58.380189Z","shell.execute_reply":"2025-05-05T10:17:58.384578Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class PixelNorm(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.epsilon = 1e-8\n\n    def forward(self, x):\n        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:17:59.050438Z","iopub.execute_input":"2025-05-05T10:17:59.050720Z","iopub.status.idle":"2025-05-05T10:17:59.055080Z","shell.execute_reply.started":"2025-05-05T10:17:59.050699Z","shell.execute_reply":"2025-05-05T10:17:59.054378Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Conv2dBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_pixelnorm = True):\n        super().__init__()\n        self.conv1 = WSConv2d(in_channels, out_channels)\n        self.conv2 = WSConv2d(out_channels, out_channels)\n        self.leaky = nn.LeakyReLU(.2)\n        self.pix = PixelNorm()\n        self.use_pix = use_pixelnorm\n        \n    def forward(self, x):\n        x = self.leaky(self.conv1(x))\n        x = self.pix(x) if self.use_pix else x\n        x = self.leaky(self.conv2(x))\n        x = self.pix(x) if self.use_pix else x\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:18:01.126774Z","iopub.execute_input":"2025-05-05T10:18:01.127483Z","iopub.status.idle":"2025-05-05T10:18:01.132160Z","shell.execute_reply.started":"2025-05-05T10:18:01.127458Z","shell.execute_reply":"2025-05-05T10:18:01.131427Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# **Create Generator**","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self,z_dim, in_channels, image_channels = 3):\n        super().__init__()\n        self.initial = nn.Sequential(\n            WSConvTranspose2d(z_dim, in_channels, kernel_size = 4, stride = 1, padding = 0),\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, in_channels, 3, 1, 1),\n            nn.LeakyReLU(0.2),\n            PixelNorm(),\n        )\n\n        self.initial_rgb = WSConv2d(in_channels, image_channels, kernel_size = 1, stride = 1, padding = 0)\n        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([self.initial_rgb])\n\n        for i in range(len(factors) - 1):\n            conv_in_c = int(in_channels * factors[i])\n            conv_out_c = int(in_channels * factors[i+1])\n            self.prog_blocks.append(Conv2dBlock(conv_in_c, conv_out_c))\n            self.rgb_layers.append(WSConv2d(conv_out_c, image_channels, kernel_size = 1, stride = 1, padding = 0))\n\n    def fade(self, alpha, upscaled, generated):\n        return torch.tanh(alpha * generated + (1 - alpha)*upscaled)\n\n    def forward(self, x, alpha, steps):\n        out = self.initial(x)\n        if steps == 0:\n            return self.initial_rgb(out)\n            \n        for step in range(steps):\n            upscaled = F.interpolate(out, scale_factor = 2, mode='nearest')\n            out = self.prog_blocks[step](upscaled)\n            \n        final_upscaled = self.rgb_layers[steps-1](upscaled)\n        final_out = self.rgb_layers[steps](out)\n        \n        return self.fade(alpha, final_upscaled, final_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:18:01.917118Z","iopub.execute_input":"2025-05-05T10:18:01.917734Z","iopub.status.idle":"2025-05-05T10:18:01.925020Z","shell.execute_reply.started":"2025-05-05T10:18:01.917708Z","shell.execute_reply":"2025-05-05T10:18:01.924248Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# **Create Discriminator**","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels, image_channels = 3):\n        super().__init__()    \n        self.initial_rgb = WSConv2d(image_channels, in_channels, kernel_size = 1, stride = 1,padding = 0)\n        self.avgpool = nn.AvgPool2d(kernel_size = 2, stride = 2)\n        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()\n        self.leaky = nn.LeakyReLU(.2)\n        factors_reversed = list(reversed(factors))\n        for i in range(len(factors_reversed) - 1,):\n            conv_in_c = int(in_channels * factors_reversed[i])\n            conv_out_c = int(in_channels * factors_reversed[i+1])\n            self.prog_blocks.append(Conv2dBlock(conv_in_c, conv_out_c, use_pixelnorm = False))\n            self.rgb_layers.append(WSConv2d(image_channels, conv_in_c, kernel_size = 1, stride = 1,padding = 0))\n\n        self.rgb_layers.append(self.initial_rgb)\n        self.final_block = nn.Sequential(\n            WSConv2d(in_channels + 1, in_channels, kernel_size = 3, padding = 1),\n            nn.LeakyReLU(.2),\n            WSConv2d(in_channels, in_channels, kernel_size = 4, stride = 1, padding = 0),\n            nn.LeakyReLU(.2),\n            nn.Flatten(),\n            nn.Linear(in_channels, 1)\n            # WSConv2d(in_channels, 1, kernel_size = 1, stride = 1, padding = 0),\n        )\n        \n\n    def fade(self, alpha, downscaled, out):\n        return alpha * out + (1 - alpha)*downscaled\n\n\n    def mini_batch(self, x):\n        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n        return torch.cat([x, batch_statistics], dim=1)\n        \n\n    \n    def forward(self, x, alpha, steps):\n        current_step = len(self.prog_blocks) - steps\n        out = self.leaky(self.rgb_layers[current_step](x))\n\n        if steps == 0:\n            out = self.mini_batch(out)\n            return self.final_block(out).view(out.shape[0], -1)\n\n        downscaled = self.leaky(self.rgb_layers[current_step + 1](self.avgpool(x)))\n        out = self.avgpool(self.prog_blocks[current_step](out))\n        out = self.fade(alpha, downscaled, out)\n\n        for step in range(current_step +1, len(self.prog_blocks)):\n            out = self.prog_blocks[step](out)\n            out = self.avgpool(out)\n        out = self.mini_batch(out)\n        out = self.final_block(out)\n        return out\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:18:06.347508Z","iopub.execute_input":"2025-05-05T10:18:06.348082Z","iopub.status.idle":"2025-05-05T10:18:06.358298Z","shell.execute_reply.started":"2025-05-05T10:18:06.348062Z","shell.execute_reply":"2025-05-05T10:18:06.357566Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def test_model():\n    z_dim = 50\n    in_channels = 512\n    gen = Generator(z_dim, in_channels)\n    disc = Discriminator(in_channels)\n    for img_size in [4,8,16,32,64,128,256,512,1024]:\n        num_steps = int(log2(img_size/4))\n        x = torch.randn(1, z_dim, 1, 1)\n        z = gen(x, .5, steps = num_steps)\n        assert z.shape == (1, 3, img_size, img_size)\n        out = disc(z, .5, steps = num_steps)\n        assert out.shape == (1,1)\ntest_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:18:06.715771Z","iopub.execute_input":"2025-05-05T10:18:06.716080Z","iopub.status.idle":"2025-05-05T10:18:15.242238Z","shell.execute_reply.started":"2025-05-05T10:18:06.716062Z","shell.execute_reply":"2025-05-05T10:18:15.241688Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/385655861.py:32: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n  batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# **Load and Augment Data**","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndataset = ImageFolder('/kaggle/input/celebahq/celeba_hq/train')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:18:19.441770Z","iopub.execute_input":"2025-05-05T10:18:19.442004Z","iopub.status.idle":"2025-05-05T10:19:15.297905Z","shell.execute_reply.started":"2025-05-05T10:18:19.441989Z","shell.execute_reply":"2025-05-05T10:19:15.297133Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/celebahq/celeba_hq'\n# Initialize Hyperparameters\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nLEARNING_RATE = 1e-3\nIMAGE_SIZE = 512\nCHANNELS_IMG = 3\nZ_DIM = 256\nIN_CHANNELS = 256\nLAMDA_GP = 10\nNUM_STEPS = int(log2(IMAGE_SIZE/4)) + 1\nNUM_EPOCH = 200\nNUM_WORKERS = 4\nBATCH_SIZES = [16, 16, 16, 16, 16, 16, 16, 8, 4]\nPROGRESSIVE_EPOCHS = [10] * len(BATCH_SIZES)\nFIXED_NOISE = torch.randn(1, Z_DIM, 1, 1).to(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:54:56.007913Z","iopub.execute_input":"2025-05-05T10:54:56.008184Z","iopub.status.idle":"2025-05-05T10:54:56.013675Z","shell.execute_reply.started":"2025-05-05T10:54:56.008164Z","shell.execute_reply":"2025-05-05T10:54:56.012920Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Loader Function\ndef get_loader(image_size, BATCH_SIZES):\n    transform = transforms.Compose([\n        transforms.Resize((image_size,image_size)),\n        transforms.ToTensor(),\n        transforms.RandomHorizontalFlip(),\n        transforms.Normalize([0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)])\n    ])\n    batch_size = BATCH_SIZES[int(log2(image_size/4))]\n    dataset = ImageFolder(root=ROOT_DIR, transform=transform)\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True\n    )\n    return loader, dataset\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:54:56.446347Z","iopub.execute_input":"2025-05-05T10:54:56.446657Z","iopub.status.idle":"2025-05-05T10:54:56.451725Z","shell.execute_reply.started":"2025-05-05T10:54:56.446637Z","shell.execute_reply":"2025-05-05T10:54:56.450925Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"# **Create Loss Functions**","metadata":{}},{"cell_type":"code","source":"def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n    BATCH_SIZE, C, H, W = real.shape\n    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n    interpolated_images = real * beta + fake.detach() * (1 - beta)\n    interpolated_images.requires_grad_(True)\n\n    # Calculate critic scores\n    mixed_scores = critic(interpolated_images, alpha, train_step)\n\n    # Take the gradient of the scores with respect to the images\n    gradient = torch.autograd.grad(\n        inputs=interpolated_images,\n        outputs=mixed_scores,\n        grad_outputs=torch.ones_like(mixed_scores),\n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    gradient = gradient.view(gradient.shape[0], -1)\n    gradient_norm = gradient.norm(2, dim=1)\n    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n    return gradient_penalty","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:54:59.156039Z","iopub.execute_input":"2025-05-05T10:54:59.156796Z","iopub.status.idle":"2025-05-05T10:54:59.161935Z","shell.execute_reply.started":"2025-05-05T10:54:59.156771Z","shell.execute_reply":"2025-05-05T10:54:59.161071Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Initialize Models\ngen = Generator(Z_DIM, IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\ncritic = Discriminator(IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:54:59.460352Z","iopub.execute_input":"2025-05-05T10:54:59.460665Z","iopub.status.idle":"2025-05-05T10:54:59.648587Z","shell.execute_reply.started":"2025-05-05T10:54:59.460644Z","shell.execute_reply":"2025-05-05T10:54:59.647998Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Initialize Optimizers\nopt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\nopt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:55:11.804807Z","iopub.execute_input":"2025-05-05T10:55:11.805082Z","iopub.status.idle":"2025-05-05T10:55:11.810697Z","shell.execute_reply.started":"2025-05-05T10:55:11.805062Z","shell.execute_reply":"2025-05-05T10:55:11.810006Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"os.makedirs('images',exist_ok = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:55:12.094624Z","iopub.execute_input":"2025-05-05T10:55:12.094922Z","iopub.status.idle":"2025-05-05T10:55:12.098763Z","shell.execute_reply.started":"2025-05-05T10:55:12.094902Z","shell.execute_reply":"2025-05-05T10:55:12.098026Z"}},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":"# **Model Training**","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n# create train function\ndef train(epochs, critic, gen, opt_critic, opt_gen, BATCH_SIZES,device):\n    critic.train()\n    gen.train()\n    step = 0\n    for num_epochs in epochs[step:]:\n        alpha = 1e-5\n        image_size_ = 4*2**step\n        loader, dataset = get_loader(image_size_, BATCH_SIZES)\n        for epoch in range(num_epochs):\n            epoch_loss_critic = 0  # Accumulate discriminator loss for the epoch\n            epoch_loss_gen = 0   # Accumulate generator loss for the epoch\n            with tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as t:\n                for batch_idx, (real, _) in enumerate(t):\n                    real = real.to(device)\n                    cur_batch_size = real.shape[0]\n                    \n                    \n                    # Train critic\n                    noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n                    fake = gen(noise, alpha, step)\n                    critic_real = critic(real, alpha, step)\n                    critic_fake = critic(fake.detach(), alpha, step )\n                    gp = gradient_penalty(critic, real, fake,alpha, step, device=device)\n                    loss_critic = (\n                    -(torch.mean(critic_real) - torch.mean(critic_fake)) \n                    + gp * LAMDA_GP\n                    + (0.001 * torch.mean(critic_real ** 2))\n                    )\n                    critic.zero_grad()\n                    loss_critic.backward()\n                    opt_critic.step()\n                    \n                    \n                    # Train Generator\n                    gen_fake = critic(fake, alpha, step)\n                    loss_gen = -torch.mean(gen_fake)\n                    gen.zero_grad()\n                    loss_gen.backward()\n                    opt_gen.step()\n\n                    alpha += cur_batch_size/(len(dataset)) * epochs[step]*0.5\n                    alpha = min(alpha,1)\n                    epoch_loss_critic += loss_critic.item()\n                    epoch_loss_gen += loss_gen.item()\n                    t.set_postfix(d_loss=loss_critic.item(), g_loss=loss_gen.item())\n                 # Average loss over all batches in the epoch\n            avg_loss_critic = epoch_loss_critic / len(loader)\n            avg_loss_gen = epoch_loss_gen / len(loader)\n               # Log progress at the end of each epoch\n            print(\n                f\"Epoch [{epoch+1}/{num_epochs}] | \"\n                f\"Avg D Loss: {avg_loss_critic:.4f} | Avg G Loss: {avg_loss_gen:.4f}\"\n            )\n            with torch.no_grad():\n                generated_image = gen(FIXED_NOISE, alpha, step) *0.5 +0.5\n                save_image(generated_image, f\"/kaggle/working/images/generated_image{step}.png\")\n            print(image_size_)\n\n        step = step + 1\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:55:12.799889Z","iopub.execute_input":"2025-05-05T10:55:12.800441Z","iopub.status.idle":"2025-05-05T10:55:12.810668Z","shell.execute_reply.started":"2025-05-05T10:55:12.800412Z","shell.execute_reply":"2025-05-05T10:55:12.809835Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"train(PROGRESSIVE_EPOCHS, critic, gen, opt_critic, opt_gen, BATCH_SIZES, DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:55:15.979223Z","iopub.execute_input":"2025-05-05T10:55:15.979816Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f32b7d103a4477bd9ff4604fa17ebb"}},"metadata":{}},{"name":"stdout","text":"Epoch [1/10] | Avg D Loss: -0.5085 | Avg G Loss: 1.0963\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74009962c8324ac4a210680d67a14cde"}},"metadata":{}},{"name":"stdout","text":"Epoch [2/10] | Avg D Loss: -0.1612 | Avg G Loss: 0.5253\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5737486c04544fa9eda6880b161f54a"}},"metadata":{}},{"name":"stdout","text":"Epoch [3/10] | Avg D Loss: -0.1637 | Avg G Loss: 0.3149\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da734979248f42cc8f60d36313a71ad7"}},"metadata":{}},{"name":"stdout","text":"Epoch [4/10] | Avg D Loss: -0.1572 | Avg G Loss: 0.2744\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b315b44d282f490683728db4ec15b03b"}},"metadata":{}},{"name":"stdout","text":"Epoch [5/10] | Avg D Loss: -0.1401 | Avg G Loss: 0.2575\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b701e18addf143fe8d2cbb932c39d5aa"}},"metadata":{}},{"name":"stdout","text":"Epoch [6/10] | Avg D Loss: -0.1142 | Avg G Loss: 0.2320\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d9c9c7cf9c14716b033715200e90491"}},"metadata":{}},{"name":"stdout","text":"Epoch [7/10] | Avg D Loss: -0.0835 | Avg G Loss: 0.2072\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c3a365b0f244668dc369fef330f5c3"}},"metadata":{}},{"name":"stdout","text":"Epoch [8/10] | Avg D Loss: -0.0421 | Avg G Loss: 0.1544\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"553dc0e5f05148cab364fa29e6c6851c"}},"metadata":{}},{"name":"stdout","text":"Epoch [9/10] | Avg D Loss: -0.0299 | Avg G Loss: 0.1536\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cb7dc6fa1094addb474c9105b701b15"}},"metadata":{}},{"name":"stdout","text":"Epoch [10/10] | Avg D Loss: -0.0313 | Avg G Loss: 0.1480\n4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/1875 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b8b818c45694c77ae8607c8f41e9e49"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}